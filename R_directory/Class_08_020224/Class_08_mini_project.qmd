---
title: "Class_08_020124"
author: "Johann Tailor"
format: pdf
editor: visual
---

# Goal

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

Our data will be sourced from the site:

\*\*Sometimes the data is not a url, in that case you can download it in the directory and then launch it using `read.csv()` or use the following code chunk:

```{r}
wisc.df <- read.csv(url("https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv"), row.names = 1)


```

> Q1: How many observations/samples/patient# are in your data? Answer: 569

You can use this also (in-text running code):

`r nrow(wisc.df)`

```{r}
nrow(wisc.df)
```

ANSWER: 569

> Q2: Whats in the `$diagnosis` column? How many of each types? Answer: Benign: 357 M: 212

Ways you can do this: Calculate T/F and count?

You can also use the table function:

```{r}
sum(wisc.df$diagnosis == "M")
sum(wisc.df$diagnosis == "B")


#the best one:
table(wisc.df$diagnosis)
```

> Q3. How many variables/features in the data are suffixed with \_mean?

Answer: 10

```{r}

grep("_mean", colnames(wisc.df))

length(grep("_mean", colnames(wisc.df)))
```

We will save the diagnosis for later:

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
diagnosis 
```

We will now delete the diagnosis column so that we dont know the answer.

```{r}
wisc.data <- wisc.df [,-1]
dim(wisc.data)
```

# Section 2: Using PCA

Let's try clustering this data:

The format: hclust(d, method = "complete", members = NULL)

```{r}
wisc.hc <- hclust(dist(wisc.data))
plot(wisc.hc)
```

The data as is when clustered doesn't look good.

Let's try PCA

But first lets see if we have to scale the data.

```{r}

apply(mtcars, 2, sd)

```

In this example, display since its ST.DEV is very high, it will dominate the whole PCA. Therefore, we need to scale it.

```{r}
pc.scale <- prcomp(mtcars, scale=T)
summary(pc.scale)
biplot(pc.scale)
```

## Back to cancer dataset:

```{r}
apply(wisc.data,2 , sd)


```

We see that the variance is very different so we will scale it.

```{r}
wisc.pc.scale <- prcomp(wisc.data, scale=T)

```

How well is the PCs captured from the original data set:

```{r}
summary(wisc.pc.scale)
```

Now, lets get our main PC score plot (a.k.a PC1 Vs. PC2 plot):

```{r}
# these are the attributes of the PCA plot. They will be standard.

attributes(wisc.pc.scale)
```

```{r}
plot(wisc.pc.scale$x[, 1], wisc.pc.scale$x[, 2], col=diagnosis)

# 1 here stands for PC1 nad 2 stands for PC2.

```

Now, lets make nice ggplot

```{r}


pc <- as.data.frame(wisc.pc.scale$x)
dim(pc)


```

```{r}
library(ggplot2)

ggplot(pc, aes(x= pc$PC1, y= pc$PC2, col=diagnosis)) + geom_point() + labs(x = "PC1 (44.27%)", y = "PC2 (18.97%)")


```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

Answer: 44.27%

```{r}
summary(wisc.pc.scale)

```

ANSWER: It cover 44.27% of the variance.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Answer: PC1 and PC2 cover about 63.24% (closest to 70%). The summary shown above was used to calculate it.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

PC1 through PC6 cover 88.759% of data (closest to 90%)

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

Answer: Its very crowded and has all patient infomration. It needs to be put in terms of variance via PCA plots.

```{r}
biplot(wisc.pc.scale)
```

```{r}

plot(wisc.pc.scale$x, col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

Answer: they are very closely clustered. No start or visual difference.

Use this: plot(wisc.pc.scale$x[, 1], wisc.pc.scale$x\[, 2\], col=diagnosis)

```{r}
plot(wisc.pc.scale$x[, 1], wisc.pc.scale$x[, 3], col=diagnosis,
     xlab = "PC1", ylab = "PC3")
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr\$rotation\[,1\]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.


Answer: -0.2608538
```{r}
loading_component <- wisc.pc.scale$rotation["concave.points_mean", 1]

# Print the result
print(loading_component)
```

## hierarchical clustering

```{r}

#squaring the standard deviation for each column:

wisc.pc.scale$sdev^2

#saving the variance of each PC as pv.rar
pr.var <- wisc.pc.scale$sdev^2
head(pr.var)

#pve will divide each PC with the total variance 
pve <- (pr.var)/ (sum(pr.var))
pve



```

Plotting the scree plot:

```{r}
# Plot variance explained for each principal component

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Another way:

```{r}

library(factoextra)
fviz_eig(wisc.pc.scale, addlabels = TRUE)
```


> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

Answer: at the height of 35, I get 4 clusters as shown by the graph below.

```{r}

# taking onlt the first three PCs

#wisc.pc.scale$x[,1:3]

wisc.pr.hclust <- hclust(dist (wisc.pc.scale$x[,1:3]), method = "ward.D2")

plot(wisc.pr.hclust)

#lets cut the dendogram to get bigger clusters:

abline(h=35, col="red")

```

> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

Answer: After looking at them all, I think `ward.D2` shows the data in an understandable manner. It shows clear clusters and the data is represented in botttom up heirachchial manner.


## combining methods

Here we will use the results of the PCA as a the input to a clustering analysis:

```{r}

# taking onlt the first three PCs

#wisc.pc.scale$x[,1:3]

wisc.pr.hclust <- hclust(dist (wisc.pc.scale$x[,1:3]), method = "ward.D2")

plot(wisc.pr.hclust)

#lets cut the dendogram to get bigger clusters:

abline(h=35, col="red")

```

```{r}
groups <- cutree(wisc.pr.hclust, k=2)
table(groups)

```

```{r}
table(groups, diagnosis)
```
```{r}
plot(wisc.pc.scale$x[,1:2], col=groups)
```


> Q.Q11. OPTIONAL: Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? How do you judge the quality of your result in each case?

```{r}
#now lets find how many patients are involved in these two groups:

table(groups)

table(diagnosis)

#This will combine the two data sets to give you a cross-reference:

table(groups, diagnosis)
```

changing groups into clusters:

```{r}
g <- as.factor(groups)
levels(g)

g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pc.scale$x[,1:2], col=g)
```
Cut this hierarchical clustering model into 2 clusters and assign the results to wisc.pr.hclust.clusters.

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
> Q13. How well does the newly created model with four clusters separate out the two diagnoses?

Answer: The newly created model with 2 clusters is far more accurate than the previous one w/o clustering. 

> Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}

table(wisc.pr.hclust.clusters, diagnosis)

```


## Section 5

179/212 = sensitivity True Negative = NON MALIGNANT

## Section 6: Prediction


```{r}

#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pc.scale, newdata=new)
npc
```

```{r}
plot(wisc.pc.scale$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")


```


>Q16. Which of these new patients should we prioritize for follow up based on your results?

Answer: I think patient 1, as its clustering is very solid with the black group and it has a PC1 value that is positive (~2.5)
